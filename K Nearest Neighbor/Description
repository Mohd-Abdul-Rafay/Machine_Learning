Implementation of k-Nearest Neighbors for Digit Recognition

(Note, you are not to use modules which provide these functions - that would be too easy (no sklearn.naive_bayes, for example) 
but rather create them yourselves.
The dataset to use is the digit recognition dataset available from the sklearn module.

k-Nearest Neighbors:

Each digit is an 8x8 pixel patch, which when reshaped is a 64 length vector.

Distance metric: the simplest distance metric for k-Nearest Neighbors is the sum of squares error between pixel values.

For each example in the test set, calculate the distance to every other example in the training set.
Identify the closest k neighbors (a good use of numpy.argsort). Pick the class which most of the neighbors belong to.

Compare the true class of each member of the test set to the predicted class using k-Nearest neighbors. Report the accuracy.

Now, change the value of k. Create a table with the accuracy for k=1, k=3, k=5, and k=100, and k=500.
Show a classification matrix for each run of k. You can use sklearn.metrics.confusion_matrix for this.
Note which errors are more common. In what way does that match your intuitions?
